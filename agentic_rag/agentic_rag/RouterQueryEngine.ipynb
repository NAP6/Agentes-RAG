{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1324e38fa4243a0",
   "metadata": {},
   "source": [
    "# Router Query Engine\n",
    "\n",
    "#### Esta lección muestra cómo usar LlamaIndex para crear un Agente RAG simple que use un Router Query Engine \n",
    "para responder preguntas de un conjunto de documentos.\n",
    "\n",
    "Para esto, explicaremos cada una de las partes que necesitamos para construir el Agente RAG, y como cada uno de esto\n",
    "pasos influyen en el resultado final y funcionamiento de nuestro modelo.\n",
    "\n",
    "#### Paso 1: Preparar el entorno de ejecución\n",
    "\n",
    "Para simplificar este ejercicio, usaremos uno de los modelos de OpenAI, por lo que necesitamos una llave API para poder\n",
    "acceder a sus servicios. Esta debe encontrarse en un archivo .env en la raíz del proyecto, o en su defecto, debe estar\n",
    "ya en las variables de entorno de su sistema operativo.\n",
    "\n",
    "Sí se encuentra en un archivo .env, ejecutamos la siguiente celda para cargar las variables de entorno:"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "908595fc2d6d52dc",
   "metadata": {},
   "source": [
    "Dado que nos encontramos en un entorno de Jupyter, y que LlamaIndex usa asyncio para diveras funcionalidades usaremos **nest_asyncio**."
   ]
  },
  {
   "cell_type": "code",
   "id": "c1e88e5781fa21c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:12:30.157458Z",
     "start_time": "2024-08-13T18:12:30.153556Z"
    }
   },
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "b7079d340969aabd",
   "metadata": {},
   "source": [
    "#### Paso 2: Cargar los documentos\n",
    "\n",
    "Para cargar documentos utilizando LlamaIndex, tienes varias opciones dependiendo de tus necesidades y la estructura de tus datos. \n",
    "Aquí te explico dos métodos comunes:\n",
    "\n",
    "##### 1. Cargar documentos automáticamente con **SimpleDirectoryReader**\n",
    "Si tus documentos están almacenados en un directorio y quieres cargarlos automáticamente, puedes utilizar **SimpleDirectoryReader**. \n",
    "Este método es útil cuando tienes muchos archivos de texto en una carpeta y deseas cargarlos todos de una vez.\n",
    "\n",
    "```python\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "# Ruta al directorio donde están almacenados tus documentos\n",
    "path_to_documents = \"./data\"\n",
    "\n",
    "# Cargar todos los documentos del directorio especificado\n",
    "documents = SimpleDirectoryReader(path_to_documents).load_data()\n",
    "```\n",
    "\n",
    "##### 2. Crear documentos manualmente\n",
    "Si prefieres tener un control más detallado sobre cómo se crean tus documentos, o si necesitas incluir metadatos específicos en el \n",
    "momento de la creación, puedes construir los documentos manualmente.\n",
    "\n",
    "```python\n",
    "from llama_index import Document\n",
    "\n",
    "# Lista de textos que quieres convertir en documentos\n",
    "text_list = [\"Texto del documento 1\", \"Texto del documento 2\", ...]\n",
    "\n",
    "# Crear documentos manualmente\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "```\n",
    "\n",
    "##### Personalización adicional\n",
    "**Añadir metadatos**\n",
    "Puedes añadir metadatos útiles en el momento de la creación del documento para facilitar la indexación y recuperación posterior.\n",
    "\n",
    "```python\n",
    "document = Document(\n",
    "    text=\"Ejemplo de texto del documento\",\n",
    "    metadata={\"filename\": \"nombre_del_archivo.txt\", \"category\": \"categoría\"}\n",
    ")\n",
    "```\n",
    "\n",
    "**Establecer ID del documento**\n",
    "Si necesitas gestionar y referenciar documentos específicos de manera eficiente, puedes establecer un identificador único para \n",
    "cada documento.\n",
    "\n",
    "```python\n",
    "document.doc_id = \"ID_único_del_documento\"\n",
    "```\n",
    "\n",
    "##### 3. Carga de documentos desde una base de datos\n",
    "Si tus documentos están almacenados en una base de datos, puedes escribir un script para extraerlos y cargarlos en LlamaIndex. Esto es útil cuando los documentos \n",
    "ya están organizados y accesibles a través de sistemas de gestión de bases de datos.\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "from llama_index import Document\n",
    "\n",
    "# Conectar a la base de datos SQLite\n",
    "conn = sqlite3.connect('tu_base_de_datos.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Consultar los documentos\n",
    "cursor.execute(\"SELECT texto FROM documentos\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Cargar los documentos en LlamaIndex\n",
    "documents = [Document(text=row[0]) for row in rows]\n",
    "```\n",
    "\n",
    "##### 4. Integración con APIs\n",
    "Si los documentos están disponibles a través de una API, puedes escribir un script que haga solicitudes a la API, reciba los documentos y los cargue en LlamaIndex. \n",
    "Esto es especialmente útil para documentos que se actualizan con frecuencia o están en plataformas de terceros.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from llama_index import Document\n",
    "\n",
    "# URL de la API que devuelve documentos\n",
    "api_url = 'https://api.ejemplo.com/documentos'\n",
    "\n",
    "# Hacer la solicitud a la API\n",
    "response = requests.get(api_url)\n",
    "data = response.json()\n",
    "\n",
    "# Cargar los documentos en LlamaIndex\n",
    "documents = [Document(text=d['contenido']) for d in data['documentos']]\n",
    "```\n",
    "\n",
    "##### 5. Procesamiento y transformación de documentos\n",
    "Antes de cargar los documentos en LlamaIndex, es posible que desees procesarlos o transformarlos para mejorar la calidad de la indexación o adaptarlos a tus necesidades \n",
    "específicas. Esto puede incluir la eliminación de etiquetas HTML, la corrección ortográfica, o la extracción de información específica.\n",
    "\n",
    "```python\n",
    "from llama_index import Document\n",
    "import re\n",
    "\n",
    "# Función para limpiar HTML\n",
    "def limpiar_html(texto):\n",
    "    return re.sub('<[^<]+?>', '', texto)\n",
    "\n",
    "# Lista de documentos HTML\n",
    "html_docs = [\"<p>Documento 1</p>\", \"<div>Documento 2</div>\"]\n",
    "\n",
    "# Limpiar y cargar documentos\n",
    "documents = [Document(text=limpiar_html(doc)) for doc in html_docs]\n",
    "```\n",
    "\n",
    "Estos métodos adicionales te ofrecen flexibilidad para adaptar la carga y gestión de documentos a diferentes entornos y fuentes de datos, maximizando la eficiencia y efectividad de tu implementación de LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "id": "119b257c0f3445d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:12:42.161535Z",
     "start_time": "2024-08-13T18:12:35.856170Z"
    }
   },
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load lora_paper.pdf documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"./documents/lora_paper.pdf\"]).load_data()"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "48d5e296f81f7d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:12:45.202795Z",
     "start_time": "2024-08-13T18:12:45.197483Z"
    }
   },
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "print(documents[2].get_content(metadata_mode=MetadataMode.LLM))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 3\n",
      "file_path: documents\\lora_paper.pdf\n",
      "\n",
      "During full ﬁne-tuning, the model is initialized to pre-trained weights Φ0and updated to Φ0+ ∆Φ\n",
      "by repeatedly following the gradient to maximize the conditional language modeling objective:\n",
      "max\n",
      "Φ∑\n",
      "(x,y)∈Z|y|∑\n",
      "t=1log(PΦ(yt|x,y<t)) (1)\n",
      "One of the main drawbacks for full ﬁne-tuning is that for each downstream task, we learn a different\n",
      "set of parameters ∆Φwhose dimension|∆Φ|equals|Φ0|. Thus, if the pre-trained model is large\n",
      "(such as GPT-3 with |Φ0|≈175Billion), storing and deploying many independent instances of\n",
      "ﬁne-tuned models can be challenging, if at all feasible.\n",
      "In this paper, we adopt a more parameter-efﬁcient approach, where the task-speciﬁc parameter\n",
      "increment ∆Φ = ∆Φ(Θ) is further encoded by a much smaller-sized set of parameters Θwith\n",
      "|Θ|≪| Φ0|. The task of ﬁnding ∆Φthus becomes optimizing over Θ:\n",
      "max\n",
      "Θ∑\n",
      "(x,y)∈Z|y|∑\n",
      "t=1log(\n",
      "pΦ0+∆Φ(Θ) (yt|x,y<t))\n",
      "(2)\n",
      "In the subsequent sections, we propose to use a low-rank representation to encode ∆Φthat is both\n",
      "compute- and memory-efﬁcient. When the pre-trained model is GPT-3 175B, the number of train-\n",
      "able parameters|Θ|can be as small as 0.01% of|Φ0|.\n",
      "3 A REN’TEXISTING SOLUTIONS GOOD ENOUGH ?\n",
      "The problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens\n",
      "of works have sought to make model adaptation more parameter- and compute-efﬁcient. See Sec-\n",
      "tion 6 for a survey of some of the well-known works. Using language modeling as an example, there\n",
      "are two prominent strategies when it comes to efﬁcient adaptations: adding adapter layers (Houlsby\n",
      "et al., 2019; Rebufﬁ et al., 2017; Pfeiffer et al., 2021; R ¨uckl´e et al., 2020) or optimizing some forms\n",
      "of the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\n",
      "Liu et al., 2021). However, both strategies have their limitations, especially in a large-scale and\n",
      "latency-sensitive production scenario.\n",
      "Adapter Layers Introduce Inference Latency There are many variants of adapters. We focus\n",
      "on the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\n",
      "and a more recent one by Lin et al. (2020) which has only one per block but with an additional\n",
      "LayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\n",
      "ing multi-task settings (R ¨uckl´e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\n",
      "the extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\n",
      "to have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\n",
      "mension, which limits the FLOPs they can add. However, large neural networks rely on hardware\n",
      "parallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\n",
      "a difference in the online inference setting where the batch size is typically as small as one. In a\n",
      "generic scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\n",
      "medium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\n",
      "very small bottleneck dimension (Table 1).\n",
      "This problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\n",
      "ikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\n",
      "AllReduce andBroadcast , unless we store the adapter parameters redundantly many times.\n",
      "Directly Optimizing the Prompt is Hard The other direction, as exempliﬁed by preﬁx tuning (Li\n",
      "& Liang, 2021), faces a different challenge. We observe that preﬁx tuning is difﬁcult to optimize\n",
      "and that its performance changes non-monotonically in trainable parameters, conﬁrming similar\n",
      "observations in the original paper. More fundamentally, reserving a part of the sequence length for\n",
      "adaptation necessarily reduces the sequence length available to process a downstream task, which\n",
      "we suspect makes tuning the prompt less performant compared to other methods. We defer the study\n",
      "on task performance to Section 5.\n",
      "3\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "4d32060ba9854c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:12:50.139054Z",
     "start_time": "2024-08-13T18:12:50.055224Z"
    }
   },
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# chunk_size of 1024 is a good default value\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# Create nodes from documents\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "fea3032bc11cb700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:12:52.555248Z",
     "start_time": "2024-08-13T18:12:52.549473Z"
    }
   },
   "source": [
    "node_metadata = nodes[1].get_content(metadata_mode=True)\n",
    "print(node_metadata)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 2\n",
      "file_name: lora_paper.pdf\n",
      "file_path: documents\\lora_paper.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1609513\n",
      "creation_date: 2024-08-13\n",
      "last_modified_date: 2024-08-13\n",
      "\n",
      "often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\n",
      "depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\n",
      "bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\n",
      "match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.\n",
      "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\n",
      "over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\n",
      "change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\n",
      "Low-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\n",
      "network indirectly by optimizing rank decomposition matrices of the dense layers’ change during\n",
      "adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n",
      "175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufﬁces even\n",
      "when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.\n",
      "LoRA possesses several key advantages.\n",
      "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
      "ferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\n",
      "matricesAandBin Figure 1, reducing the storage requirement and task-switching over-\n",
      "head signiﬁcantly.\n",
      "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\n",
      "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
      "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
      "much smaller low-rank matrices.\n",
      "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\n",
      "when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\n",
      "construction.\n",
      "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\n",
      "as preﬁx-tuning. We provide an example in Appendix E.\n",
      "Terminologies and Conventions We make frequent references to the Transformer architecture\n",
      "and use the conventional terminologies for its dimensions. We call the input and output di-\n",
      "mension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\n",
      "query/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\n",
      "trained weight matrix and ∆Wits accumulated gradient update during adaptation. We use rto\n",
      "denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\n",
      "Brown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\n",
      "optimization and use a Transformer MLP feedforward dimension dffn= 4×dmodel .\n",
      "2 P ROBLEM STATEMENT\n",
      "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\n",
      "ing use case. Below is a brief description of the language modeling problem and, in particular, the\n",
      "maximization of conditional probabilities given a task-speciﬁc prompt.\n",
      "Suppose we are given a pre-trained autoregressive language model PΦ(y|x)parametrized by Φ.\n",
      "For instance, PΦ(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\n",
      "et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\n",
      "pre-trained model to downstream conditional text generation tasks, such as summarization, machine\n",
      "reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\n",
      "represented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\n",
      "yiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\n",
      "corresponding SQL command; for summarization, xiis the content of an article and yiits summary.\n",
      "2\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "9c4d6ec931338b9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:12:55.856850Z",
     "start_time": "2024-08-13T18:12:55.847161Z"
    }
   },
   "source": [
    "from llama_index.core import set_global_service_context\n",
    "from lib.ollama_config import service_context_ollama\n",
    "from lib.openai_config import service_context_openai\n",
    "\n",
    "# set_global_service_context(service_context_ollama)\n",
    "set_global_service_context(service_context_openai)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "fe3ec1c635b18d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:14:14.192928Z",
     "start_time": "2024-08-13T18:14:11.973418Z"
    }
   },
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "# summary index\n",
    "summary_index = SummaryIndex(nodes)\n",
    "# vector store index\n",
    "vector_index = VectorStoreIndex(nodes, vervose=True)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "2e4d8f8e6fd5ee11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:14:14.572335Z",
     "start_time": "2024-08-13T18:14:14.567076Z"
    }
   },
   "source": [
    "# summary query engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    "    vervose=True,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# vector query engine\n",
    "vector_query_engine = vector_index.as_query_engine(vervose=True, streaming=True)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "8a0bb4d0a242d97c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:14:17.998768Z",
     "start_time": "2024-08-13T18:14:17.994791Z"
    }
   },
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the Lora paper.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the the Lora paper.\"\n",
    "    ),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "b5f2c6e01ef1e663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:14:21.280330Z",
     "start_time": "2024-08-13T18:14:21.275008Z"
    }
   },
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "b6019a06d7935944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:14:57.737275Z",
     "start_time": "2024-08-13T18:14:42.702608Z"
    }
   },
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;3;38;5;200mSelecting query engine 0: The question is asking for a summary of the document, which is typically related to summarization questions..\n",
      "\u001B[0mThe document discusses a novel adaptation strategy called LoRA, which introduces trainable rank decomposition matrices into each layer of large language models like GPT-3. This approach aims to reduce the number of trainable parameters for downstream tasks while addressing the challenges of full fine-tuning. Empirical investigations on rank-deficiency in language model adaptation are explored, showing that LoRA performs on-par or better than traditional fine-tuning methods on various language models. The document also delves into experiments and analyses related to model adaptation, low-rank matrices, and subspace similarity in deep learning models, examining the impact of different adaptation methods on tasks like MNLI, WikiSQL, and E2E NLG Challenge. Insights into the performance and behavior of these methods in different experimental settings are provided, along with discussions on practical benefits, limitations, and future research directions related to LoRA.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b9faf4d50ccd2770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:39:44.147363Z",
     "start_time": "2024-08-13T14:39:19.386710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El documento parece ser un artículo técnico sobre modelado de lenguaje y adaptación de modelos a tareas específicas. Se discute la utilidad de técnicas como el fine-tuning y la imposición de estructuras de bajo rango para mejorar el desempeño de los modelos en tareas de procesamiento de lenguaje natural.\n"
     ]
    }
   ],
   "source": [
    "res = vector_query_engine.query(\"Cual es el resumen del Documento?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c00270dfc26cc24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:15:51.135342Z",
     "start_time": "2024-08-13T18:15:42.125745Z"
    }
   },
   "source": [
    "res = summary_query_engine.query(\"Cual es el resumen del Documento?\")\n",
    "print(res)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El documento presenta un enfoque llamado LoRA (Low-Rank Adaptation) para adaptar eficientemente modelos de lenguaje a tareas específicas sin introducir latencia adicional en la inferencia. LoRA congela los pesos del modelo pre-entrenado y agrega matrices de descomposición de rango entrenables en cada capa de la arquitectura Transformer, reduciendo el número de parámetros entrenables. Se demuestra que LoRA supera o iguala el rendimiento del ajuste fino en modelos como RoBERTa, DeBERTa, GPT-2 y GPT-3, a pesar de tener menos parámetros entrenables y una mayor eficiencia de entrenamiento. Además, se discuten experimentos adicionales realizados en diferentes modelos de lenguaje, explorando aspectos como la correlación entre módulos de adaptación, el efecto de la variable r en GPT-2, y la amplificación de direcciones específicas de tareas en matrices de baja dimensión, junto con análisis detallados de similitudes entre subespacios y resultados de experimentos en diversas configuraciones de modelos y tareas específicas.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "54c88078bcf9711d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:17:17.438456Z",
     "start_time": "2024-08-13T18:17:15.758448Z"
    }
   },
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "print(Settings.llm.complete(\"Quiern eres? y quien te creo\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soy un asistente de inteligencia artificial creado por un equipo de desarrolladores de OpenAI. Mi propósito es ayudar a responder preguntas y brindar información útil a los usuarios.\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
