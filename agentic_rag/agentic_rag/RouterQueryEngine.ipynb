{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1324e38fa4243a0",
   "metadata": {},
   "source": [
    "# Router Query Engine\n",
    "\n",
    "#### Esta lección muestra cómo usar LlamaIndex para crear un Agente RAG simple que use un Router Query Engine para responder preguntas de un conjunto de documentos.\n",
    "\n",
    "Para esto, explicaremos cada una de las partes que necesitamos para construir el Agente RAG, y como cada uno de estos\n",
    "pasos influyen en el resultado final y funcionamiento de nuestro modelo.\n",
    "\n",
    "#### Paso 1: Preparar el entorno de ejecución\n",
    "\n",
    "Para simplificar este ejercicio, usaremos uno de los modelos de OpenAI, por lo que necesitamos una llave API para poder\n",
    "acceder a sus servicios. Esta debe encontrarse en un archivo .env en la raíz del proyecto, o en su defecto, debe estar\n",
    "ya en las variables de entorno de su sistema operativo.\n",
    "\n",
    "Sí se encuentra en un archivo .env, ejecutamos la siguiente celda para cargar las variables de entorno:"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:43:10.325917Z",
     "start_time": "2024-08-13T19:43:10.312477Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "908595fc2d6d52dc",
   "metadata": {},
   "source": [
    "Dado que nos encontramos en un entorno de Jupyter, y que LlamaIndex usa asyncio para diveras funcionalidades usaremos **nest_asyncio**."
   ]
  },
  {
   "cell_type": "code",
   "id": "c1e88e5781fa21c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:43:19.214359Z",
     "start_time": "2024-08-13T19:43:19.209686Z"
    }
   },
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "b7079d340969aabd",
   "metadata": {},
   "source": [
    "#### Paso 2: Cargar los documentos\n",
    "\n",
    "Para cargar documentos utilizando LlamaIndex, tienes varias opciones dependiendo de tus necesidades y la estructura de tus datos. \n",
    "Aquí te explico dos métodos comunes:\n",
    "\n",
    "##### 1. Cargar documentos automáticamente con **SimpleDirectoryReader**\n",
    "Si tus documentos están almacenados en un directorio y quieres cargarlos automáticamente, puedes utilizar **SimpleDirectoryReader**. \n",
    "Este método es útil cuando tienes muchos archivos de texto en una carpeta y deseas cargarlos todos de una vez.\n",
    "\n",
    "```python\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "# Ruta al directorio donde están almacenados tus documentos\n",
    "path_to_documents = \"./data\"\n",
    "\n",
    "# Cargar todos los documentos del directorio especificado\n",
    "documents = SimpleDirectoryReader(path_to_documents).load_data()\n",
    "```\n",
    "\n",
    "##### 2. Crear documentos manualmente\n",
    "Si prefieres tener un control más detallado sobre cómo se crean tus documentos, o si necesitas incluir metadatos específicos en el \n",
    "momento de la creación, puedes construir los documentos manualmente.\n",
    "\n",
    "```python\n",
    "from llama_index import Document\n",
    "\n",
    "# Lista de textos que quieres convertir en documentos\n",
    "text_list = [\"Texto del documento 1\", \"Texto del documento 2\", ...]\n",
    "\n",
    "# Crear documentos manualmente\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "```\n",
    "\n",
    "##### Personalización adicional\n",
    "**Añadir metadatos**\n",
    "Puedes añadir metadatos útiles en el momento de la creación del documento para facilitar la indexación y recuperación posterior.\n",
    "\n",
    "```python\n",
    "document = Document(\n",
    "    text=\"Ejemplo de texto del documento\",\n",
    "    metadata={\"filename\": \"nombre_del_archivo.txt\", \"category\": \"categoría\"}\n",
    ")\n",
    "```\n",
    "\n",
    "**Establecer ID del documento**\n",
    "Si necesitas gestionar y referenciar documentos específicos de manera eficiente, puedes establecer un identificador único para \n",
    "cada documento.\n",
    "\n",
    "```python\n",
    "document.doc_id = \"ID_único_del_documento\"\n",
    "```\n",
    "\n",
    "##### 3. Carga de documentos desde una base de datos\n",
    "Si tus documentos están almacenados en una base de datos, puedes escribir un script para extraerlos y cargarlos en LlamaIndex. Esto es útil cuando los documentos \n",
    "ya están organizados y accesibles a través de sistemas de gestión de bases de datos.\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "from llama_index import Document\n",
    "\n",
    "# Conectar a la base de datos SQLite\n",
    "conn = sqlite3.connect('tu_base_de_datos.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Consultar los documentos\n",
    "cursor.execute(\"SELECT texto FROM documentos\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Cargar los documentos en LlamaIndex\n",
    "documents = [Document(text=row[0]) for row in rows]\n",
    "```\n",
    "\n",
    "##### 4. Integración con APIs\n",
    "Si los documentos están disponibles a través de una API, puedes escribir un script que haga solicitudes a la API, reciba los documentos y los cargue en LlamaIndex. \n",
    "Esto es especialmente útil para documentos que se actualizan con frecuencia o están en plataformas de terceros.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from llama_index import Document\n",
    "\n",
    "# URL de la API que devuelve documentos\n",
    "api_url = 'https://api.ejemplo.com/documentos'\n",
    "\n",
    "# Hacer la solicitud a la API\n",
    "response = requests.get(api_url)\n",
    "data = response.json()\n",
    "\n",
    "# Cargar los documentos en LlamaIndex\n",
    "documents = [Document(text=d['contenido']) for d in data['documentos']]\n",
    "```\n",
    "\n",
    "##### 5. Procesamiento y transformación de documentos\n",
    "Antes de cargar los documentos en LlamaIndex, es posible que desees procesarlos o transformarlos para mejorar la calidad de la indexación o adaptarlos a tus necesidades \n",
    "específicas. Esto puede incluir la eliminación de etiquetas HTML, la corrección ortográfica, o la extracción de información específica.\n",
    "\n",
    "```python\n",
    "from llama_index import Document\n",
    "import re\n",
    "\n",
    "# Función para limpiar HTML\n",
    "def limpiar_html(texto):\n",
    "    return re.sub('<[^<]+?>', '', texto)\n",
    "\n",
    "# Lista de documentos HTML\n",
    "html_docs = [\"<p>Documento 1</p>\", \"<div>Documento 2</div>\"]\n",
    "\n",
    "# Limpiar y cargar documentos\n",
    "documents = [Document(text=limpiar_html(doc)) for doc in html_docs]\n",
    "```\n",
    "\n",
    "Estos métodos adicionales te ofrecen flexibilidad para adaptar la carga y gestión de documentos a diferentes entornos y fuentes de datos, maximizando la eficiencia y efectividad de tu implementación de LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "id": "119b257c0f3445d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:43:35.017738Z",
     "start_time": "2024-08-13T19:43:29.510114Z"
    }
   },
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load lora_paper.pdf documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"./documents/lora_paper.pdf\"]).load_data()"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "48d5e296f81f7d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:43:41.615624Z",
     "start_time": "2024-08-13T19:43:41.610773Z"
    }
   },
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "print(documents[2].get_content(metadata_mode=MetadataMode.LLM))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 3\n",
      "file_path: documents\\lora_paper.pdf\n",
      "\n",
      "During full ﬁne-tuning, the model is initialized to pre-trained weights Φ0and updated to Φ0+ ∆Φ\n",
      "by repeatedly following the gradient to maximize the conditional language modeling objective:\n",
      "max\n",
      "Φ∑\n",
      "(x,y)∈Z|y|∑\n",
      "t=1log(PΦ(yt|x,y<t)) (1)\n",
      "One of the main drawbacks for full ﬁne-tuning is that for each downstream task, we learn a different\n",
      "set of parameters ∆Φwhose dimension|∆Φ|equals|Φ0|. Thus, if the pre-trained model is large\n",
      "(such as GPT-3 with |Φ0|≈175Billion), storing and deploying many independent instances of\n",
      "ﬁne-tuned models can be challenging, if at all feasible.\n",
      "In this paper, we adopt a more parameter-efﬁcient approach, where the task-speciﬁc parameter\n",
      "increment ∆Φ = ∆Φ(Θ) is further encoded by a much smaller-sized set of parameters Θwith\n",
      "|Θ|≪| Φ0|. The task of ﬁnding ∆Φthus becomes optimizing over Θ:\n",
      "max\n",
      "Θ∑\n",
      "(x,y)∈Z|y|∑\n",
      "t=1log(\n",
      "pΦ0+∆Φ(Θ) (yt|x,y<t))\n",
      "(2)\n",
      "In the subsequent sections, we propose to use a low-rank representation to encode ∆Φthat is both\n",
      "compute- and memory-efﬁcient. When the pre-trained model is GPT-3 175B, the number of train-\n",
      "able parameters|Θ|can be as small as 0.01% of|Φ0|.\n",
      "3 A REN’TEXISTING SOLUTIONS GOOD ENOUGH ?\n",
      "The problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens\n",
      "of works have sought to make model adaptation more parameter- and compute-efﬁcient. See Sec-\n",
      "tion 6 for a survey of some of the well-known works. Using language modeling as an example, there\n",
      "are two prominent strategies when it comes to efﬁcient adaptations: adding adapter layers (Houlsby\n",
      "et al., 2019; Rebufﬁ et al., 2017; Pfeiffer et al., 2021; R ¨uckl´e et al., 2020) or optimizing some forms\n",
      "of the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\n",
      "Liu et al., 2021). However, both strategies have their limitations, especially in a large-scale and\n",
      "latency-sensitive production scenario.\n",
      "Adapter Layers Introduce Inference Latency There are many variants of adapters. We focus\n",
      "on the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\n",
      "and a more recent one by Lin et al. (2020) which has only one per block but with an additional\n",
      "LayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\n",
      "ing multi-task settings (R ¨uckl´e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\n",
      "the extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\n",
      "to have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\n",
      "mension, which limits the FLOPs they can add. However, large neural networks rely on hardware\n",
      "parallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\n",
      "a difference in the online inference setting where the batch size is typically as small as one. In a\n",
      "generic scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\n",
      "medium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\n",
      "very small bottleneck dimension (Table 1).\n",
      "This problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\n",
      "ikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\n",
      "AllReduce andBroadcast , unless we store the adapter parameters redundantly many times.\n",
      "Directly Optimizing the Prompt is Hard The other direction, as exempliﬁed by preﬁx tuning (Li\n",
      "& Liang, 2021), faces a different challenge. We observe that preﬁx tuning is difﬁcult to optimize\n",
      "and that its performance changes non-monotonically in trainable parameters, conﬁrming similar\n",
      "observations in the original paper. More fundamentally, reserving a part of the sequence length for\n",
      "adaptation necessarily reduces the sequence length available to process a downstream task, which\n",
      "we suspect makes tuning the prompt less performant compared to other methods. We defer the study\n",
      "on task performance to Section 5.\n",
      "3\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Paso 3: Fragmentar los documentos\n",
    "\n",
    "Los LLM existentes a día de hoy tienen dos problemas que impiden pasar un libro completo al modelo para\n",
    "poder hacerle preguntas sobre este. La primera es la ventana de contexto, que limita la cantidad de texto\n",
    "que se le puede pasar al modelo, y la segunda es que el modelo se condicionara con base en lo que le pasemos, por lo que\n",
    "si tiene demasiada información con una pobre relacion con la pregunta que le hagamos, es muy probable que el modelo se\n",
    "confunda y no nos dé una respuesta correcta.\n",
    "\n",
    "Para esto se fragmentan los documentos en elementos más pequeños, que puedan ser manejados por el modelo, además, esto \n",
    "puede ser util para indexarlos y encontrarlos mucho más rapido. Agregarle metadatos, que ayuden a encontrar y recuperar\n",
    "los fragmeos (nodos-chunks) de manera más eficiente, y además permitan al modelo tratarlos de manera más eficiente."
   ],
   "id": "3a2f42e8a6f24636"
  },
  {
   "cell_type": "code",
   "id": "4d32060ba9854c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:43:42.995721Z",
     "start_time": "2024-08-13T19:43:42.928582Z"
    }
   },
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# chunk_size of 1024 is a good default value\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# Create nodes from documents\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "fea3032bc11cb700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:43:44.072859Z",
     "start_time": "2024-08-13T19:43:44.067557Z"
    }
   },
   "source": [
    "node_metadata = nodes[1].get_content(metadata_mode=True)\n",
    "print(node_metadata)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 2\n",
      "file_name: lora_paper.pdf\n",
      "file_path: documents\\lora_paper.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1609513\n",
      "creation_date: 2024-08-13\n",
      "last_modified_date: 2024-08-13\n",
      "\n",
      "often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\n",
      "depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\n",
      "bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\n",
      "match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.\n",
      "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\n",
      "over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\n",
      "change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\n",
      "Low-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\n",
      "network indirectly by optimizing rank decomposition matrices of the dense layers’ change during\n",
      "adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n",
      "175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufﬁces even\n",
      "when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.\n",
      "LoRA possesses several key advantages.\n",
      "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
      "ferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\n",
      "matricesAandBin Figure 1, reducing the storage requirement and task-switching over-\n",
      "head signiﬁcantly.\n",
      "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\n",
      "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
      "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
      "much smaller low-rank matrices.\n",
      "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\n",
      "when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\n",
      "construction.\n",
      "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\n",
      "as preﬁx-tuning. We provide an example in Appendix E.\n",
      "Terminologies and Conventions We make frequent references to the Transformer architecture\n",
      "and use the conventional terminologies for its dimensions. We call the input and output di-\n",
      "mension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\n",
      "query/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\n",
      "trained weight matrix and ∆Wits accumulated gradient update during adaptation. We use rto\n",
      "denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\n",
      "Brown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\n",
      "optimization and use a Transformer MLP feedforward dimension dffn= 4×dmodel .\n",
      "2 P ROBLEM STATEMENT\n",
      "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\n",
      "ing use case. Below is a brief description of the language modeling problem and, in particular, the\n",
      "maximization of conditional probabilities given a task-speciﬁc prompt.\n",
      "Suppose we are given a pre-trained autoregressive language model PΦ(y|x)parametrized by Φ.\n",
      "For instance, PΦ(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\n",
      "et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\n",
      "pre-trained model to downstream conditional text generation tasks, such as summarization, machine\n",
      "reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\n",
      "represented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\n",
      "yiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\n",
      "corresponding SQL command; for summarization, xiis the content of an article and yiits summary.\n",
      "2\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Paso 4: Definimos la configuración de los servicios que utilizaremos\n",
    "\n",
    "Dependiendo de si se utilizaran los servicios de OpenAI servicios en local, en servidores on-premise o en la nube,\n",
    "hay que configurar los servicios que se utilizaran, para conectarse al LLM, asi como a su servicio de embeddings, y\n",
    "demás."
   ],
   "id": "302547bdfe107c60"
  },
  {
   "cell_type": "code",
   "id": "9c4d6ec931338b9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T12:14:47.582254Z",
     "start_time": "2024-08-16T12:14:46.573393Z"
    }
   },
   "source": [
    "from llama_index.core import set_global_service_context\n",
    "from my_lib.ollama_config import service_context_ollama\n",
    "from my_lib.openai_config import service_context_openai\n",
    "\n",
    "# set_global_service_context(service_context_ollama)\n",
    "set_global_service_context(service_context_openai)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Paso 5: Crear los índices\n",
    "\n",
    "Los índices son estructuras de datos que permiten la rapida recuperacion de contexto para\n",
    "responder a las preguntas de los usuarios. La diferencia entre uno u otro está en como \n",
    "almacena la información y como la recupera.\n",
    "\n",
    "En este caso, usaremos dos tipos de índices, uno de resumen, que almacena los nodos en forma le\n",
    " lista y al momento de recuperarlo, va sobre cada uno de los elementos paso a paso resumiendo y\n",
    " respondiendo con cada fragmento la pregunta; y otro de almacenamiento en forma de vector,\n",
    " que genera embeddings de los textos, que son representaciones numericas que almacenan el contexto."
   ],
   "id": "364c4e642b5cbdc4"
  },
  {
   "cell_type": "code",
   "id": "fe3ec1c635b18d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:44:19.294268Z",
     "start_time": "2024-08-13T19:44:17.918261Z"
    }
   },
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "# summary index\n",
    "summary_index = SummaryIndex(nodes)\n",
    "# vector store index\n",
    "vector_index = VectorStoreIndex(nodes, vervose=True)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Paso 6: Crear los motores de consulta\n",
    "\n",
    "Los motores de consulta se montan sobre los índices y retrivers, este se encarga de tratar las preguntas, \n",
    "y recuperar el contexto para responderlas."
   ],
   "id": "1f4eae75c290db3d"
  },
  {
   "cell_type": "code",
   "id": "2e4d8f8e6fd5ee11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:44:20.882962Z",
     "start_time": "2024-08-13T19:44:20.879263Z"
    }
   },
   "source": [
    "# summary query engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    "    vervose=True,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# vector query engine\n",
    "vector_query_engine = vector_index.as_query_engine(vervose=True, streaming=True)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Paso 7: Crear las herramientas de consulta\n",
    "\n",
    "Dado que vamos a intentar crear un Enrutador de consulta es necesario crear las Query Tools, que seran las encargadas\n",
    "de responder a las preguntas del usuario, pero utilizando las mejores herramientas a base de su description. El trabajo\n",
    "de razonamiento y decision de la mejor herramienta lo se encargará el enrutador."
   ],
   "id": "10edc5f22238c467"
  },
  {
   "cell_type": "code",
   "id": "8a0bb4d0a242d97c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:40:48.647249Z",
     "start_time": "2024-08-14T14:40:48.641939Z"
    }
   },
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the Lora paper.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the the Lora paper.\"\n",
    "    ),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "b5f2c6e01ef1e663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:44:25.185526Z",
     "start_time": "2024-08-13T19:44:25.179411Z"
    }
   },
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ejecutar consultas\n",
    "\n",
    "A continuacion se realizan consultas a los motores de consulta, para ver como responden a las preguntas.\n",
    "En esta se puede observar como responde la misma pregunta, el motor de resumen y el motor de vectores, y \n",
    "cuál es el que elige el enrutador."
   ],
   "id": "a579ef7e7a2091cf"
  },
  {
   "cell_type": "code",
   "id": "b6019a06d7935944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:44:40.345761Z",
     "start_time": "2024-08-13T19:44:31.705198Z"
    }
   },
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;3;38;5;200mSelecting query engine 0: The question is asking for a summary of the document, which is typically related to summarization questions..\n",
      "\u001B[0mThe document introduces a novel adaptation strategy called LoRA for large language models like GPT-3. LoRA involves freezing pre-trained model weights and introducing trainable rank decomposition matrices to reduce the number of trainable parameters for downstream tasks. This approach aims to overcome the challenges of full fine-tuning by significantly reducing the number of trainable parameters while maintaining or improving model quality. The study includes empirical investigations into rank-deficiency in language model adaptation and highlights the benefits of LoRA in terms of storage efficiency, training speed, and task-switching capabilities. Additionally, the document explores various experiments and analyses related to deep learning models, focusing on adaptation methods like LoRA, preﬁx-tuning, and low-rank matrices, and examines the impact of different hyperparameters on model performance across different tasks.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "b9faf4d50ccd2770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:44:58.574079Z",
     "start_time": "2024-08-13T19:44:55.914843Z"
    }
   },
   "source": [
    "res = vector_query_engine.query(\"Cual es el resumen del Documento?\")\n",
    "print(res)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El documento discute la eficiencia de un método llamado LoRA en comparación con otras técnicas, incluido el ajuste fino, al evaluar un conjunto de validación completo. Se menciona una métrica de proyección llamada Ham & Lee (2008) y se presentan fórmulas relacionadas con los valores singulares de matrices. Además, se hace referencia a otros trabajos relacionados con adaptadores en transformers, factorización de matrices de rango bajo y entrenamiento de modelos de lenguaje con paralelismo de modelos.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "4c00270dfc26cc24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:45:07.901297Z",
     "start_time": "2024-08-13T19:44:58.575085Z"
    }
   },
   "source": [
    "res = summary_query_engine.query(\"Cual es el resumen del Documento?\")\n",
    "print(res)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El documento presenta un enfoque llamado LoRA (Low-Rank Adaptation) para la adaptación eficiente de modelos de lenguaje a tareas específicas sin introducir latencia adicional en la inferencia. LoRA congela los pesos del modelo pre-entrenado y agrega matrices de descomposición de rango entrenables en cada capa, lo que reduce significativamente el número de parámetros entrenables. Este enfoque ha demostrado ser efectivo en mantener la calidad del modelo en tareas como RoBERTa, DeBERTa, GPT-2 y GPT-3, incluso con menos parámetros entrenables y una mayor eficiencia de entrenamiento. Además, se presentan experimentos adicionales que exploran aspectos como la correlación entre los módulos de adaptación, el efecto de la variación de r (rango) en GPT-2, el factor de amplificación, y la medición de la similitud entre subespacios, así como los resultados obtenidos en experimentos sobre matrices de rango bajo.\n"
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
