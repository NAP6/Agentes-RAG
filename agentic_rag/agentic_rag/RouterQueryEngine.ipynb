{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Router Query Engine\n",
    "\n",
    "#### Esta lección muestra cómo usar LlamaIndex para crear un Agente RAG simple que use un Router Query Engine \n",
    "para responder preguntas de un conjunto de documentos.\n",
    "\n",
    "Para esto, explicaremos cada una de las partes que necesitamos para construir el Agente RAG, y como cada uno de esto\n",
    "pasos influyen en el resultado final y funcionamiento de nuestro modelo.\n",
    "\n",
    "#### Paso 1: Preparar el entorno de ejecución\n",
    "\n",
    "Para simplificar este ejercicio, usaremos uno de los modelos de OpenAI, por lo que necesitamos una llave API para poder\n",
    "acceder a sus servicios. Esta debe encontrarse en un archivo .env en la raíz del proyecto, o en su defecto, debe estar\n",
    "ya en las variables de entorno de su sistema operativo.\n",
    "\n",
    "Sí se encuentra en un archivo .env, ejecutamos la siguiente celda para cargar las variables de entorno:"
   ],
   "id": "b1324e38fa4243a0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-02T06:39:15.644018Z",
     "start_time": "2024-08-02T06:39:15.418210Z"
    }
   },
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dado que nos encontramos en un entorno de Jupyter, y que LlamaIndex usa asyncio para diveras funcionalidades usaremos **nest_asyncio**.",
   "id": "908595fc2d6d52dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "id": "c1e88e5781fa21c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Paso 2: Cargar los documentos\n",
    "\n",
    "Para cargar documentos utilizando LlamaIndex, tienes varias opciones dependiendo de tus necesidades y la estructura de tus datos. \n",
    "Aquí te explico dos métodos comunes:\n",
    "\n",
    "##### 1. Cargar documentos automáticamente con **SimpleDirectoryReader**\n",
    "Si tus documentos están almacenados en un directorio y quieres cargarlos automáticamente, puedes utilizar **SimpleDirectoryReader**. \n",
    "Este método es útil cuando tienes muchos archivos de texto en una carpeta y deseas cargarlos todos de una vez.\n",
    "\n",
    "```python\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "# Ruta al directorio donde están almacenados tus documentos\n",
    "path_to_documents = \"./data\"\n",
    "\n",
    "# Cargar todos los documentos del directorio especificado\n",
    "documents = SimpleDirectoryReader(path_to_documents).load_data()\n",
    "```\n",
    "\n",
    "##### 2. Crear documentos manualmente\n",
    "Si prefieres tener un control más detallado sobre cómo se crean tus documentos, o si necesitas incluir metadatos específicos en el \n",
    "momento de la creación, puedes construir los documentos manualmente.\n",
    "\n",
    "```python\n",
    "from llama_index import Document\n",
    "\n",
    "# Lista de textos que quieres convertir en documentos\n",
    "text_list = [\"Texto del documento 1\", \"Texto del documento 2\", ...]\n",
    "\n",
    "# Crear documentos manualmente\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "```\n",
    "\n",
    "##### Personalización adicional\n",
    "**Añadir metadatos**\n",
    "Puedes añadir metadatos útiles en el momento de la creación del documento para facilitar la indexación y recuperación posterior.\n",
    "\n",
    "```python\n",
    "document = Document(\n",
    "    text=\"Ejemplo de texto del documento\",\n",
    "    metadata={\"filename\": \"nombre_del_archivo.txt\", \"category\": \"categoría\"}\n",
    ")\n",
    "```\n",
    "\n",
    "**Establecer ID del documento**\n",
    "Si necesitas gestionar y referenciar documentos específicos de manera eficiente, puedes establecer un identificador único para \n",
    "cada documento.\n",
    "\n",
    "```python\n",
    "document.doc_id = \"ID_único_del_documento\"\n",
    "```\n",
    "\n",
    "##### 3. Carga de documentos desde una base de datos\n",
    "Si tus documentos están almacenados en una base de datos, puedes escribir un script para extraerlos y cargarlos en LlamaIndex. Esto es útil cuando los documentos \n",
    "ya están organizados y accesibles a través de sistemas de gestión de bases de datos.\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "from llama_index import Document\n",
    "\n",
    "# Conectar a la base de datos SQLite\n",
    "conn = sqlite3.connect('tu_base_de_datos.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Consultar los documentos\n",
    "cursor.execute(\"SELECT texto FROM documentos\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Cargar los documentos en LlamaIndex\n",
    "documents = [Document(text=row[0]) for row in rows]\n",
    "```\n",
    "\n",
    "##### 4. Integración con APIs\n",
    "Si los documentos están disponibles a través de una API, puedes escribir un script que haga solicitudes a la API, reciba los documentos y los cargue en LlamaIndex. \n",
    "Esto es especialmente útil para documentos que se actualizan con frecuencia o están en plataformas de terceros.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from llama_index import Document\n",
    "\n",
    "# URL de la API que devuelve documentos\n",
    "api_url = 'https://api.ejemplo.com/documentos'\n",
    "\n",
    "# Hacer la solicitud a la API\n",
    "response = requests.get(api_url)\n",
    "data = response.json()\n",
    "\n",
    "# Cargar los documentos en LlamaIndex\n",
    "documents = [Document(text=d['contenido']) for d in data['documentos']]\n",
    "```\n",
    "\n",
    "##### 5. Procesamiento y transformación de documentos\n",
    "Antes de cargar los documentos en LlamaIndex, es posible que desees procesarlos o transformarlos para mejorar la calidad de la indexación o adaptarlos a tus necesidades \n",
    "específicas. Esto puede incluir la eliminación de etiquetas HTML, la corrección ortográfica, o la extracción de información específica.\n",
    "\n",
    "```python\n",
    "from llama_index import Document\n",
    "import re\n",
    "\n",
    "# Función para limpiar HTML\n",
    "def limpiar_html(texto):\n",
    "    return re.sub('<[^<]+?>', '', texto)\n",
    "\n",
    "# Lista de documentos HTML\n",
    "html_docs = [\"<p>Documento 1</p>\", \"<div>Documento 2</div>\"]\n",
    "\n",
    "# Limpiar y cargar documentos\n",
    "documents = [Document(text=limpiar_html(doc)) for doc in html_docs]\n",
    "```\n",
    "\n",
    "Estos métodos adicionales te ofrecen flexibilidad para adaptar la carga y gestión de documentos a diferentes entornos y fuentes de datos, maximizando la eficiencia y efectividad de tu implementación de LlamaIndex."
   ],
   "id": "b7079d340969aabd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load lora_paper.pdf documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"./documents/lora_paper.pdf\"]).load_data()"
   ],
   "id": "119b257c0f3445d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T09:29:20.290333Z",
     "start_time": "2024-08-02T09:29:20.284977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "print(documents[4].get_content(metadata_mode=MetadataMode.LLM))"
   ],
   "id": "48d5e296f81f7d63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 5\n",
      "file_path: documents\\lora_paper.pdf\n",
      "\n",
      "guarantees that we do not introduce any additional latency during inference compared to a ﬁne-tuned\n",
      "model by construction.\n",
      "4.2 A PPLYING LORA TOTRANSFORMER\n",
      "In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\n",
      "number of trainable parameters. In the Transformer architecture, there are four weight matrices in\n",
      "the self-attention module ( Wq,Wk,Wv,Wo) and two in the MLP module. We treat Wq(orWk,Wv)\n",
      "as a single matrix of dimension dmodel×dmodel , even though the output dimension is usually sliced\n",
      "into attention heads. We limit our study to only adapting the attention weights for downstream\n",
      "tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\n",
      "and parameter-efﬁciency.We further study the effect on adapting different types of attention weight\n",
      "matrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\n",
      "layers, LayerNorm layers, and biases to a future work.\n",
      "Practical Beneﬁts and Limitations. The most signiﬁcant beneﬁt comes from the reduction in\n",
      "memory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\n",
      "usage by up to 2/3ifr≪dmodel as we do not need to store the optimizer states for the frozen\n",
      "parameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\n",
      "350GB. With r= 4and only the query and value projection matrices being adapted, the checkpoint\n",
      "size is reduced by roughly 10,000 ×(from 350GB to 35MB)4. This allows us to train with signiﬁ-\n",
      "cantly fewer GPUs and avoid I/O bottlenecks. Another beneﬁt is that we can switch between tasks\n",
      "while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\n",
      "parameters. This allows for the creation of many customized models that can be swapped in and out\n",
      "on the ﬂy on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\n",
      "during training on GPT-3 175B compared to full ﬁne-tuning5as we do not need to calculate the\n",
      "gradient for the vast majority of the parameters.\n",
      "LoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\n",
      "with different AandBin a single forward pass, if one chooses to absorb AandBintoWto eliminate\n",
      "additional inference latency. Though it is possible to not merge the weights and dynamically choose\n",
      "the LoRA modules to use for samples in a batch for scenarios where latency is not critical.\n",
      "5 E MPIRICAL EXPERIMENTS\n",
      "We evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De-\n",
      "BERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown\n",
      "et al., 2020). Our experiments cover a wide range of tasks, from natural language understanding\n",
      "(NLU) to generation (NLG). Speciﬁcally, we evaluate on the GLUE (Wang et al., 2019) benchmark\n",
      "for RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com-\n",
      "parison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al.,\n",
      "2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for\n",
      "more details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\n",
      "5.1 B ASELINES\n",
      "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their\n",
      "reported numbers whenever possible. This, however, means that some baselines might only appear\n",
      "in certain experiments.\n",
      "Fine-Tuning (FT) is a common approach for adaptation. During ﬁne-tuning, the model is initialized\n",
      "to the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple\n",
      "variant is to update only some layers while freezing others. We include one such baseline reported\n",
      "in prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers ( FTTop2).\n",
      "4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\n",
      "350GB + 35MB * 100 ≈354GB as opposed to 100 * 350GB ≈35TB.\n",
      "5For GPT-3 175B, the training throughput for full ﬁne-tuning is 32.5 tokens/s per V100 GPU; with the same\n",
      "number of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA.\n",
      "5\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T06:42:15.629938Z",
     "start_time": "2024-08-02T06:42:14.396239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# chunk_size of 1024 is a good default value\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# Create nodes from documents\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ],
   "id": "4d32060ba9854c01",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T06:42:18.154645Z",
     "start_time": "2024-08-02T06:42:18.146819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "node_metadata = nodes[1].get_content(metadata_mode=True)\n",
    "print(node_metadata)"
   ],
   "id": "fea3032bc11cb700",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 2\n",
      "file_name: lora_paper.pdf\n",
      "file_path: documents\\lora_paper.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1609513\n",
      "creation_date: 2024-08-02\n",
      "last_modified_date: 2024-08-02\n",
      "\n",
      "often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\n",
      "depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\n",
      "bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\n",
      "match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.\n",
      "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\n",
      "over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\n",
      "change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\n",
      "Low-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\n",
      "network indirectly by optimizing rank decomposition matrices of the dense layers’ change during\n",
      "adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n",
      "175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufﬁces even\n",
      "when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.\n",
      "LoRA possesses several key advantages.\n",
      "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
      "ferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\n",
      "matricesAandBin Figure 1, reducing the storage requirement and task-switching over-\n",
      "head signiﬁcantly.\n",
      "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\n",
      "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
      "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
      "much smaller low-rank matrices.\n",
      "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\n",
      "when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\n",
      "construction.\n",
      "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\n",
      "as preﬁx-tuning. We provide an example in Appendix E.\n",
      "Terminologies and Conventions We make frequent references to the Transformer architecture\n",
      "and use the conventional terminologies for its dimensions. We call the input and output di-\n",
      "mension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\n",
      "query/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\n",
      "trained weight matrix and ∆Wits accumulated gradient update during adaptation. We use rto\n",
      "denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\n",
      "Brown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\n",
      "optimization and use a Transformer MLP feedforward dimension dffn= 4×dmodel .\n",
      "2 P ROBLEM STATEMENT\n",
      "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\n",
      "ing use case. Below is a brief description of the language modeling problem and, in particular, the\n",
      "maximization of conditional probabilities given a task-speciﬁc prompt.\n",
      "Suppose we are given a pre-trained autoregressive language model PΦ(y|x)parametrized by Φ.\n",
      "For instance, PΦ(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\n",
      "et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\n",
      "pre-trained model to downstream conditional text generation tasks, such as summarization, machine\n",
      "reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\n",
      "represented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\n",
      "yiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\n",
      "corresponding SQL command; for summarization, xiis the content of an article and yiits summary.\n",
      "2\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T06:42:33.781663Z",
     "start_time": "2024-08-02T06:42:33.498760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# LLM model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "# embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ],
   "id": "9c4d6ec931338b9f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fe3ec1c635b18d5d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
