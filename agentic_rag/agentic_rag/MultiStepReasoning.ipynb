{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-18T14:33:22.157097Z",
     "start_time": "2024-08-18T14:33:22.132282Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:33:22.167299Z",
     "start_time": "2024-08-18T14:33:22.158103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "id": "cbc49765bc48966d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:34:15.033375Z",
     "start_time": "2024-08-18T14:34:08.609736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load lora_paper.pdf documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"./documents/lora_paper.pdf\"]).load_data()\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# chunk_size of 1024 is a good default value\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# Create nodes from documents\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ],
   "id": "7676a4d3914849b1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:34:36.798394Z",
     "start_time": "2024-08-18T14:34:35.725430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import set_global_service_context\n",
    "from my_lib.openai_config import service_context_openai\n",
    "\n",
    "set_global_service_context(service_context_openai)"
   ],
   "id": "dac3fbf843af9a2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:34:51.887654Z",
     "start_time": "2024-08-18T14:34:49.750112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "# summary index\n",
    "summary_index = SummaryIndex(nodes)\n",
    "# vector store index\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "\n",
    "# summary query engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "# vector query engine\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ],
   "id": "b7a0678accc6ba40",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:36:03.255370Z",
     "start_time": "2024-08-18T14:36:03.251065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "llm = Settings.llm"
   ],
   "id": "ecd90114e9a0ed3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:36:20.197083Z",
     "start_time": "2024-08-18T14:36:20.192448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the Lora paper.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the the Lora paper.\"\n",
    "    ),\n",
    ")"
   ],
   "id": "6a2fa1db0236adf8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:41:38.116596Z",
     "start_time": "2024-08-18T14:41:37.222562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ],
   "id": "48ec2a5bb2ee2ac8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:42:23.269400Z",
     "start_time": "2024-08-18T14:42:12.291860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = agent.query(\n",
    "    \"Explicame que es Lora y por que se esta usando. No son suficientemente buenas las soluciones existentes?\"\n",
    ") "
   ],
   "id": "177cb80c16dec3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explicame que es Lora y por que se esta usando. No son suficientemente buenas las soluciones existentes?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"What is Lora and why is it being used? Are existing solutions not good enough?\"}\n",
      "=== Function Output ===\n",
      "Lora is a method that enhances the adaptation of large language models to specific tasks or domains efficiently by injecting trainable rank decomposition matrices into each layer of the Transformer architecture. It aims to amplify task-specific directions in pre-trained models for better adaptation to downstream tasks, emphasizing important but not emphasized directions in the original model. Lora is being used to improve model performance, reduce memory requirements, maintain model quality, and enable efficient task-switching without introducing additional latency during inference. Existing solutions like adapter layers or optimizing input layer activations have limitations such as introducing inference latency, reducing model quality, or facing challenges in optimizing prompts, which makes Lora a valuable approach for improving model adaptation, especially in low-data scenarios.\n",
      "=== LLM Response ===\n",
      "Lora is a method that enhances the adaptation of large language models to specific tasks or domains efficiently by injecting trainable rank decomposition matrices into each layer of the Transformer architecture. It aims to amplify task-specific directions in pre-trained models for better adaptation to downstream tasks, emphasizing important but not emphasized directions in the original model. Lora is being used to improve model performance, reduce memory requirements, maintain model quality, and enable efficient task-switching without introducing additional latency during inference.\n",
      "\n",
      "Existing solutions like adapter layers or optimizing input layer activations have limitations such as introducing inference latency, reducing model quality, or facing challenges in optimizing prompts. This makes Lora a valuable approach for improving model adaptation, especially in low-data scenarios where existing solutions may not be sufficient.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:43:39.243167Z",
     "start_time": "2024-08-18T14:43:39.239063Z"
    }
   },
   "cell_type": "code",
   "source": "print(str(response))",
   "id": "aa505c0ddaf81ac7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lora is a method that enhances the adaptation of large language models to specific tasks or domains efficiently by injecting trainable rank decomposition matrices into each layer of the Transformer architecture. It aims to amplify task-specific directions in pre-trained models for better adaptation to downstream tasks, emphasizing important but not emphasized directions in the original model. Lora is being used to improve model performance, reduce memory requirements, maintain model quality, and enable efficient task-switching without introducing additional latency during inference.\n",
      "\n",
      "Existing solutions like adapter layers or optimizing input layer activations have limitations such as introducing inference latency, reducing model quality, or facing challenges in optimizing prompts. This makes Lora a valuable approach for improving model adaptation, especially in low-data scenarios where existing solutions may not be sufficient.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:44:58.675975Z",
     "start_time": "2024-08-18T14:44:40.027783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = agent.chat(\n",
    "    \"Explain to me what is Lora and why it's being used. Are existing solutions not good enough?\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ],
   "id": "df9dd061fd9b4b52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me what is Lora and why it's being used. Are existing solutions not good enough?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"What is Lora and why is it being used?\"}\n",
      "=== Function Output ===\n",
      "Lora is a method used for adapting large pre-trained language models to specific tasks or domains by introducing trainable rank decomposition matrices into each layer of the Transformer architecture. It significantly reduces the number of trainable parameters for downstream tasks, making it more memory-efficient and computationally efficient. Lora is being used to fine-tune models for better performance on specific tasks without the need for extensive retraining from scratch, addressing the challenge of adapting large models like GPT-3 with a massive number of parameters in a more feasible and cost-effective manner.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Are existing solutions not good enough for Lora?\"}\n",
      "=== Function Output ===\n",
      "Existing solutions are not considered good enough for LoRA, as they often introduce inference latency and struggle to match the fine-tuning baselines, posing a trade-off between efficiency and model quality. LoRA, on the other hand, offers a more efficient adaptation strategy without introducing inference latency and maintains high model quality, outperforming or matching fine-tuning baselines while reducing the number of trainable parameters significantly.\n",
      "=== LLM Response ===\n",
      "Lora is a method used for adapting large pre-trained language models to specific tasks or domains by introducing trainable rank decomposition matrices into each layer of the Transformer architecture. It significantly reduces the number of trainable parameters for downstream tasks, making it more memory-efficient and computationally efficient. Lora is being used to fine-tune models for better performance on specific tasks without the need for extensive retraining from scratch, addressing the challenge of adapting large models like GPT-3 with a massive number of parameters in a more feasible and cost-effective manner.\n",
      "\n",
      "Existing solutions are not considered good enough for Lora, as they often introduce inference latency and struggle to match the fine-tuning baselines, posing a trade-off between efficiency and model quality. Lora, on the other hand, offers a more efficient adaptation strategy without introducing inference latency and maintains high model quality, outperforming or matching fine-tuning baselines while reducing the number of trainable parameters significantly.\n",
      "Lora is a method used for adapting large pre-trained language models to specific tasks or domains by introducing trainable rank decomposition matrices into each layer of the Transformer architecture. It significantly reduces the number of trainable parameters for downstream tasks, making it more memory-efficient and computationally efficient. Lora is being used to fine-tune models for better performance on specific tasks without the need for extensive retraining from scratch, addressing the challenge of adapting large models like GPT-3 with a massive number of parameters in a more feasible and cost-effective manner.\n",
      "\n",
      "Existing solutions are not considered good enough for Lora, as they often introduce inference latency and struggle to match the fine-tuning baselines, posing a trade-off between efficiency and model quality. Lora, on the other hand, offers a more efficient adaptation strategy without introducing inference latency and maintains high model quality, outperforming or matching fine-tuning baselines while reducing the number of trainable parameters significantly.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:45:19.251727Z",
     "start_time": "2024-08-18T14:45:18.298448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = agent.chat(\n",
    "    \"What was my last question to you?\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ],
   "id": "a5148f9ed2e989e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What was my last question to you?\n",
      "=== LLM Response ===\n",
      "Your last question was: \"Explain to me what is Lora and why it's being used. Are existing solutions not good enough?\"\n",
      "Your last question was: \"Explain to me what is Lora and why it's being used. Are existing solutions not good enough?\"\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:46:33.298613Z",
     "start_time": "2024-08-18T14:46:33.293584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ],
   "id": "ca8ff8160a847ce5",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:46:35.108357Z",
     "start_time": "2024-08-18T14:46:35.104067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "task = agent.create_task(\n",
    "    \"Explain to me what is Lora and why it's being used.\"\n",
    "    \"Are existing solutions not good enough?\"\n",
    ")"
   ],
   "id": "8140bd7a5c040aae",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:50:08.343176Z",
     "start_time": "2024-08-18T14:50:08.336398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "\n",
    "print(f\"Number of completed steps for tasksID {task.task_id} is {len(completed_steps)}\")\n",
    "\n",
    "if len(completed_steps) > 0:\n",
    "    print(completed_steps[0].output.sources[0].raw_output)"
   ],
   "id": "2fa6f935a1ce78d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of completed steps for tasksID ed01f8a8-74b7-435b-a3df-f40f01bd40ab is 2\n",
      "Lora is a method used to adapt large language models to specific tasks efficiently by freezing pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. It significantly reduces the number of trainable parameters for downstream tasks, making it more feasible to adapt large models without the need for full fine-tuning. Lora is used to reduce the computational cost and memory requirements associated with fine-tuning large models like GPT-3, while maintaining or even improving model quality on various tasks.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:50:26.476242Z",
     "start_time": "2024-08-18T14:50:26.470567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Number of completed steps for tasksID {task.task_id} is {len(upcoming_steps)}\")\n",
    "\n",
    "if len(upcoming_steps) > 0:\n",
    "    print(upcoming_steps[0].input)"
   ],
   "id": "dbe9c6e3c008a62f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of completed steps for tasksID ed01f8a8-74b7-435b-a3df-f40f01bd40ab is 1\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:50:34.044811Z",
     "start_time": "2024-08-18T14:50:31.986120Z"
    }
   },
   "cell_type": "code",
   "source": "step_output = agent.run_step(task.task_id)",
   "id": "d39ba0da4036eb68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "Lora is a method used to adapt large language models efficiently by reducing the number of trainable parameters for downstream tasks. It helps in maintaining or improving model quality while reducing computational costs and memory requirements. Existing solutions for language model adaptation may have limitations in terms of inference latency, model quality, and scalability, which is why Lora is being used to address these challenges and provide a more efficient adaptation strategy.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:50:36.665596Z",
     "start_time": "2024-08-18T14:50:36.660166Z"
    }
   },
   "cell_type": "code",
   "source": "print(step_output.is_last)",
   "id": "6451cd17ca292676",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:51:01.748294Z",
     "start_time": "2024-08-18T14:51:01.743026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "task = agent.create_task(\n",
    "    \"Explain to me what is Lora and why it's being used.\"\n",
    "    \"Are existing solutions not good enough?\"\n",
    ")"
   ],
   "id": "77bca1757d6f3732",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:51:12.378854Z",
     "start_time": "2024-08-18T14:51:08.287953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "step_output = agent.run_step(\n",
    "    task.task_id, input=\"Explain to me the dataset used to fine-tune in the Lora paper.\"\n",
    ")"
   ],
   "id": "5917c9b7783675e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me the dataset used to fine-tune in the Lora paper.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Dataset used to fine-tune in the Lora paper\"}\n",
      "=== Function Output ===\n",
      "The dataset used for fine-tuning in the LoRA paper is MultiNLI.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:53:37.752532Z",
     "start_time": "2024-08-18T14:53:36.677133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "step_output= agent.run_step(task.task_id)\n",
    "print(step_output.is_last)"
   ],
   "id": "128ba0970a09af25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "The dataset used for fine-tuning in the Lora paper is MultiNLI.\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:53:40.500568Z",
     "start_time": "2024-08-18T14:53:40.495126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = agent.finalize_response(task.task_id)\n",
    "print(str(response))"
   ],
   "id": "b84c5db6f5cbc7e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset used for fine-tuning in the Lora paper is MultiNLI.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8a85664c1f48c192"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
