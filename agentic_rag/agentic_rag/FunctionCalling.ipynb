{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:32.634085Z",
     "start_time": "2024-08-17T14:08:32.607237Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:32.645282Z",
     "start_time": "2024-08-17T14:08:32.636094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "id": "d4b45d64be745e2f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:32.653540Z",
     "start_time": "2024-08-17T14:08:32.646289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Add two numbers together\"\"\" \n",
    "    return x + y\n",
    "\n",
    "def subtract(x: int, y: int) -> int:\n",
    "    \"\"\"Subtract two numbers together\"\"\"\n",
    "    return x - y\n",
    "\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two numbers together\"\"\"\n",
    "    return x * y\n",
    "\n",
    "def divide(x: int, y: int) -> float:\n",
    "    \"\"\"Divide two numbers, x by y\"\"\"\n",
    "    return x / y\n",
    "\n",
    "def get_user_info(name: str) -> str:\n",
    "    \"\"\"Get user informatio\"\"\"\n",
    "    data = {\n",
    "        \"John\": {\n",
    "            \"age\": 25,\n",
    "            \"location\": \"New York\"\n",
    "        },\n",
    "        \"Jane\": {\n",
    "            \"age\": 22,\n",
    "            \"location\": \"San Francisco\"\n",
    "        }\n",
    "    }\n",
    "    return f\"{name} is {data[name]['age']} years old and lives in {data[name]['location']}\"\n"
   ],
   "id": "391d8e2e83bd3e56",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:38.701904Z",
     "start_time": "2024-08-17T14:08:32.654545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "addition_tool = FunctionTool.from_defaults(fn=add)\n",
    "subtraction_tool = FunctionTool.from_defaults(fn=subtract)\n",
    "multiplication_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "division_tool = FunctionTool.from_defaults(fn=divide)\n",
    "get_user_info_tool = FunctionTool.from_defaults(fn=get_user_info)\n",
    "\n",
    "tools = [addition_tool, subtraction_tool, multiplication_tool, division_tool, get_user_info_tool]"
   ],
   "id": "436a6f9e118ca7a7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:39.922481Z",
     "start_time": "2024-08-17T14:08:38.703910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import set_global_service_context\n",
    "from my_lib.openai_config import service_context_openai\n",
    "\n",
    "set_global_service_context(service_context_openai)\n"
   ],
   "id": "27f037239ea06d5a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:45.750979Z",
     "start_time": "2024-08-17T14:08:39.923125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "llm = Settings.llm\n",
    "\n",
    "response = llm.predict_and_call(\n",
    "    tools,\n",
    "    \"Add 5 and 5\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(str(response))"
   ],
   "id": "9deb6b3862a3237c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"x\": 5, \"y\": 5}\n",
      "=== Function Output ===\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:46.701638Z",
     "start_time": "2024-08-17T14:08:45.751985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = llm.predict_and_call(\n",
    "    tools,\n",
    "    \"Tell me about John\",\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "9ed214d942609b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: get_user_info with args: {\"name\": \"John\"}\n",
      "=== Function Output ===\n",
      "John is 25 years old and lives in New York\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:55.337672Z",
     "start_time": "2024-08-17T14:08:46.702650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"./documents/lora_paper.pdf\"]).load_data()"
   ],
   "id": "ff856966a24f8e8e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:55.403299Z",
     "start_time": "2024-08-17T14:08:55.338680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "node = splitter.get_nodes_from_documents(documents)"
   ],
   "id": "5196d263e3c1ae07",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:56.813875Z",
     "start_time": "2024-08-17T14:08:55.404305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(node)"
   ],
   "id": "bab409bad4fa70f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:59.928035Z",
     "start_time": "2024-08-17T14:08:56.814137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"Cuentame sobre el problema planteado y como se explica\")\n",
    "print(str(response))"
   ],
   "id": "ca046aa7de45c627",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El problema planteado se centra en la adaptación de un modelo pre-entrenado a tareas específicas de generación de texto condicional, como resumen, comprensión de lectura automática y lenguaje natural a SQL. Se parte de un modelo autoregresivo de lenguaje pre-entrenado PΦ(y|x) parametrizado por Φ, como un aprendiz multi-tarea genérico basado en la arquitectura Transformer. La adaptación implica ajustar este modelo pre-entrenado a nuevas tareas mediante un conjunto de datos de entrenamiento que consiste en pares de contexto-objetivo. Cada tarea de generación de texto condicional tiene sus propios pares de contexto-objetivo, como consultas de lenguaje natural a comandos SQL o contenido de un artículo a su resumen.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:59.933634Z",
     "start_time": "2024-08-17T14:08:59.929041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text())\n",
    "    print(\"=============Text=============\")\n",
    "    "
   ],
   "id": "a17bd0e5ce2767e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\n",
      "depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\n",
      "bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\n",
      "match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.\n",
      "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\n",
      "over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\n",
      "change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\n",
      "Low-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\n",
      "network indirectly by optimizing rank decomposition matrices of the dense layers’ change during\n",
      "adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n",
      "175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufﬁces even\n",
      "when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.\n",
      "LoRA possesses several key advantages.\n",
      "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
      "ferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\n",
      "matricesAandBin Figure 1, reducing the storage requirement and task-switching over-\n",
      "head signiﬁcantly.\n",
      "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\n",
      "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
      "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
      "much smaller low-rank matrices.\n",
      "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\n",
      "when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\n",
      "construction.\n",
      "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\n",
      "as preﬁx-tuning. We provide an example in Appendix E.\n",
      "Terminologies and Conventions We make frequent references to the Transformer architecture\n",
      "and use the conventional terminologies for its dimensions. We call the input and output di-\n",
      "mension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\n",
      "query/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\n",
      "trained weight matrix and ∆Wits accumulated gradient update during adaptation. We use rto\n",
      "denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\n",
      "Brown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\n",
      "optimization and use a Transformer MLP feedforward dimension dffn= 4×dmodel .\n",
      "2 P ROBLEM STATEMENT\n",
      "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\n",
      "ing use case. Below is a brief description of the language modeling problem and, in particular, the\n",
      "maximization of conditional probabilities given a task-speciﬁc prompt.\n",
      "Suppose we are given a pre-trained autoregressive language model PΦ(y|x)parametrized by Φ.\n",
      "For instance, PΦ(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\n",
      "et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\n",
      "pre-trained model to downstream conditional text generation tasks, such as summarization, machine\n",
      "reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\n",
      "represented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\n",
      "yiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\n",
      "corresponding SQL command; for summarization, xiis the content of an article and yiits summary.\n",
      "2\n",
      "=============Text=============\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:59.951031Z",
     "start_time": "2024-08-17T14:08:59.934644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "def vector_search_query(\n",
    "        query: str,\n",
    "        page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Conduct a vector search across an index using the following parameters:\n",
    "\n",
    "    query (str): This is the text string you want to embed and search for within the index.\n",
    "    page_numbers (List[str]): This parameter allows you to limit the search to \n",
    "    specific pages. If left empty, the search will encompass all pages in the index. \n",
    "    If page numbers are specified, the search will be filtered to only include those pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response"
   ],
   "id": "384e169965d76619",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:08:59.965821Z",
     "start_time": "2024-08-17T14:08:59.955039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_search_tool\",\n",
    "    fn=vector_search_query\n",
    ")"
   ],
   "id": "4f1ed0965afb298a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:09:03.587059Z",
     "start_time": "2024-08-17T14:08:59.966828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool], \n",
    "    \"What was mentioned about the problem statement in page 2?\", \n",
    "    verbose=True\n",
    ")"
   ],
   "id": "eabd7d785a9a562a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_search_tool with args: {\"query\": \"problem statement\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "While the proposal is independent of the training objective, the focus is on language modeling, particularly maximizing conditional probabilities based on a task-specific prompt. The scenario involves adapting a pre-trained autoregressive language model to various downstream conditional text generation tasks like summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each task is defined by a dataset of context-target pairs, where both the context (xi) and target (yi) are sequences of tokens. For instance, in NL2SQL, xi represents a natural language query and yi its corresponding SQL command, while in summarization, xi is the article content and yi is its summary.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:12:10.724879Z",
     "start_time": "2024-08-17T14:12:10.719812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text())\n",
    "    print(\"=============Text=============\")"
   ],
   "id": "819f0e54f738a7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\n",
      "depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\n",
      "bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\n",
      "match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.\n",
      "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\n",
      "over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\n",
      "change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\n",
      "Low-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\n",
      "network indirectly by optimizing rank decomposition matrices of the dense layers’ change during\n",
      "adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n",
      "175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufﬁces even\n",
      "when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.\n",
      "LoRA possesses several key advantages.\n",
      "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
      "ferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\n",
      "matricesAandBin Figure 1, reducing the storage requirement and task-switching over-\n",
      "head signiﬁcantly.\n",
      "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\n",
      "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
      "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
      "much smaller low-rank matrices.\n",
      "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\n",
      "when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\n",
      "construction.\n",
      "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\n",
      "as preﬁx-tuning. We provide an example in Appendix E.\n",
      "Terminologies and Conventions We make frequent references to the Transformer architecture\n",
      "and use the conventional terminologies for its dimensions. We call the input and output di-\n",
      "mension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\n",
      "query/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\n",
      "trained weight matrix and ∆Wits accumulated gradient update during adaptation. We use rto\n",
      "denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\n",
      "Brown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\n",
      "optimization and use a Transformer MLP feedforward dimension dffn= 4×dmodel .\n",
      "2 P ROBLEM STATEMENT\n",
      "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\n",
      "ing use case. Below is a brief description of the language modeling problem and, in particular, the\n",
      "maximization of conditional probabilities given a task-speciﬁc prompt.\n",
      "Suppose we are given a pre-trained autoregressive language model PΦ(y|x)parametrized by Φ.\n",
      "For instance, PΦ(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\n",
      "et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\n",
      "pre-trained model to downstream conditional text generation tasks, such as summarization, machine\n",
      "reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\n",
      "represented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\n",
      "yiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\n",
      "corresponding SQL command; for summarization, xiis the content of an article and yiits summary.\n",
      "2\n",
      "=============Text=============\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:13:08.992094Z",
     "start_time": "2024-08-17T14:13:08.973190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(node)\n",
    "\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the Lora paper.\"\n",
    "    ),\n",
    ")"
   ],
   "id": "764194a0761e9986",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:13:48.657298Z",
     "start_time": "2024-08-17T14:13:45.613380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What was mentioned about the problem statement in page 2?\", \n",
    "    verbose=True\n",
    ")"
   ],
   "id": "b7d5eada28ffbf8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_search_tool with args: {\"query\": \"problem statement\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "The problem statement focuses on language modeling, particularly on maximizing conditional probabilities given a task-specific prompt. It involves adapting a pre-trained autoregressive language model to downstream conditional text generation tasks like summarization, machine reading comprehension, and natural language to SQL. Each task is defined by a dataset of context-target pairs, where the goal is to generate the target text based on the provided context.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:13:54.374841Z",
     "start_time": "2024-08-17T14:13:54.370029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text()[:10])\n",
    "    print(\"=============Text=============\")"
   ],
   "id": "2613def261efe087",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "often intr\n",
      "=============Text=============\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:14:41.413647Z",
     "start_time": "2024-08-17T14:14:33.761204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"Dame un resumen del articulo.\", \n",
    "    verbose=True\n",
    ")"
   ],
   "id": "c92fd7ce891fcf34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"Dame un resumen del articulo.\"}\n",
      "=== Function Output ===\n",
      "El artículo presenta un enfoque llamado LoRA para la adaptación eficiente de modelos de lenguaje pre-entrenados a tareas específicas. LoRA implica congelar los pesos del modelo pre-entrenado y agregar matrices de descomposición de rango entrenables en cada capa, lo que reduce la cantidad de parámetros entrenables. Se demuestra que LoRA supera o iguala al ajuste fino tradicional en la calidad del modelo en diversas tareas, como RoBERTa, DeBERTa, GPT-2 y GPT-3, a pesar de tener menos parámetros entrenables y una mayor eficiencia de entrenamiento. Además, se discuten investigaciones empíricas sobre la deficiencia de rango en la adaptación de modelos de lenguaje.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T14:15:10.768572Z",
     "start_time": "2024-08-17T14:15:10.762580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text()[:10])\n",
    "    print(\"=============Text=============\")"
   ],
   "id": "802d2a80bc8bb003",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '1', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "LORA: L OW\n",
      "=============Text=============\n",
      "{'page_label': '2', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "often intr\n",
      "=============Text=============\n",
      "{'page_label': '3', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "During ful\n",
      "=============Text=============\n",
      "{'page_label': '3', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "This makes\n",
      "=============Text=============\n",
      "{'page_label': '4', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Batch Size\n",
      "=============Text=============\n",
      "{'page_label': '4', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "LoRA takes\n",
      "=============Text=============\n",
      "{'page_label': '5', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "guarantees\n",
      "=============Text=============\n",
      "{'page_label': '5', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "See Append\n",
      "=============Text=============\n",
      "{'page_label': '6', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Model & Me\n",
      "=============Text=============\n",
      "{'page_label': '6', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "We report \n",
      "=============Text=============\n",
      "{'page_label': '7', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Model & Me\n",
      "=============Text=============\n",
      "{'page_label': '7', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "We also re\n",
      "=============Text=============\n",
      "{'page_label': '8', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Model&Meth\n",
      "=============Text=============\n",
      "{'page_label': '9', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "guage mode\n",
      "=============Text=============\n",
      "{'page_label': '9', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "In theory \n",
      "=============Text=============\n",
      "{'page_label': '10', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "to maximiz\n",
      "=============Text=============\n",
      "{'page_label': '10', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Table 6 sh\n",
      "=============Text=============\n",
      "{'page_label': '11', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Subspace s\n",
      "=============Text=============\n",
      "{'page_label': '12', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "0.00.10.20\n",
      "=============Text=============\n",
      "{'page_label': '13', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "tuning. 3)\n",
      "=============Text=============\n",
      "{'page_label': '13', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "160–167, N\n",
      "=============Text=============\n",
      "{'page_label': '14', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Behrooz Gh\n",
      "=============Text=============\n",
      "{'page_label': '14', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "arXiv: 180\n",
      "=============Text=============\n",
      "{'page_label': '15', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Xiao Liu, \n",
      "=============Text=============\n",
      "{'page_label': '15', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Learning m\n",
      "=============Text=============\n",
      "{'page_label': '16', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Ashish Vas\n",
      "=============Text=============\n",
      "{'page_label': '17', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Method MNL\n",
      "=============Text=============\n",
      "{'page_label': '18', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "and STS-B \n",
      "=============Text=============\n",
      "{'page_label': '19', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Method Dat\n",
      "=============Text=============\n",
      "{'page_label': '19', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Len. 128\n",
      "T\n",
      "=============Text=============\n",
      "{'page_label': '20', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Method Dat\n",
      "=============Text=============\n",
      "{'page_label': '21', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Hyperparam\n",
      "=============Text=============\n",
      "{'page_label': '22', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Method Web\n",
      "=============Text=============\n",
      "{'page_label': '22', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "The traini\n",
      "=============Text=============\n",
      "{'page_label': '23', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Method Hyp\n",
      "=============Text=============\n",
      "{'page_label': '23', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "MNLI-\n",
      "ndes\n",
      "=============Text=============\n",
      "{'page_label': '24', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "Hyperparam\n",
      "=============Text=============\n",
      "{'page_label': '25', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "0.00.20.40\n",
      "=============Text=============\n",
      "{'page_label': '26', 'file_name': 'lora_paper.pdf', 'file_path': 'documents\\\\lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-08-13', 'last_modified_date': '2024-08-13'}\n",
      "=============Text=============\n",
      "0.00.10.20\n",
      "=============Text=============\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
